{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7dd1c07",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-04-10T06:43:52.339400Z",
     "iopub.status.busy": "2022-04-10T06:43:52.337921Z",
     "iopub.status.idle": "2022-04-10T06:43:52.354916Z",
     "shell.execute_reply": "2022-04-10T06:43:52.355362Z",
     "shell.execute_reply.started": "2022-04-10T06:38:49.234811Z"
    },
    "papermill": {
     "duration": 0.027278,
     "end_time": "2022-04-10T06:43:52.355557",
     "exception": false,
     "start_time": "2022-04-10T06:43:52.328279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nbme-score-clinical-patient-notes/sample_submission.csv\n",
      "/kaggle/input/nbme-score-clinical-patient-notes/patient_notes.csv\n",
      "/kaggle/input/nbme-score-clinical-patient-notes/features.csv\n",
      "/kaggle/input/nbme-score-clinical-patient-notes/train.csv\n",
      "/kaggle/input/nbme-score-clinical-patient-notes/test.csv\n",
      "/kaggle/input/nbme-preprocessed-sentences/patient_notes_sentences.csv\n",
      "/kaggle/input/nbme-preprocessed-sentences/__notebook_source__.ipynb\n",
      "/kaggle/input/nbme-preprocessed-sentences/data_train_sentence.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de297ea",
   "metadata": {
    "papermill": {
     "duration": 0.004882,
     "end_time": "2022-04-10T06:43:52.367596",
     "exception": false,
     "start_time": "2022-04-10T06:43:52.362714",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MLM Pre-training\n",
    "Credit to : [https://github.com/UKPLab/sentence-transformers/blob/master/examples/unsupervised_learning/MLM/train_mlm.py](http://)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b842e705",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T06:43:52.393146Z",
     "iopub.status.busy": "2022-04-10T06:43:52.392443Z",
     "iopub.status.idle": "2022-04-10T15:22:04.218626Z",
     "shell.execute_reply": "2022-04-10T15:22:04.218185Z",
     "shell.execute_reply.started": "2022-04-10T06:40:30.858361Z"
    },
    "papermill": {
     "duration": 31091.846079,
     "end_time": "2022-04-10T15:22:04.218765",
     "exception": false,
     "start_time": "2022-04-10T06:43:52.372686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a46be5948df4e97847f1ef478eb708b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2dc3a44945248a0ada4bfe96873b9f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7678a0d7d19841d7ba7258f4533c713e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save checkpoints to: MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sentences: 606664\n",
      "Dev sentences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n",
      "tokenizer config file saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/tokenizer_config.json\n",
      "Special tokens file saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 606664\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 66360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save tokenizer to: MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66360' max='66360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [66360/66360 8:37:33, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.565200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.266400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.178200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.120200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.084700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.054900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.023500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.979100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.953400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.950200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.938200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.929200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.911300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.920400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.912800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.892000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.874400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.874500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.870100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.871500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.858400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.853800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.854100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.856900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.844900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.840500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.825100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.816400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.821700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.815300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.828100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.810600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.800400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.796600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.797800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.785800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.792800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.782100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.774200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.784400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.773000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.774100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.771900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.755500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.763900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.757500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.754200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.751000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.746600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.753100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.744700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.746500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.753300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>0.737000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.734000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>0.736300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.728500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>0.729200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.727300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>0.734100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.724300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.726000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.731900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-1000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-1000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-2000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-2000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-3000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-3000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-4000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-4000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-5000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-5000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-4000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-6000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-6000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-7000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-7000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-8000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-8000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-9000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-9000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-10000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-10000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-11000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-11000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-10000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-12000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-12000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-13000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-13000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-12000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-14000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-14000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-13000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-15000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-15000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-14000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-16000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-16000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-15000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-17000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-17000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-16000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-18000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-18000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-17000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-19000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-19000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-18000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-20000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-20000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-19000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-21000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-21000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-20000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-22000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-22000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-21000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-23000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-23000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-22000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-24000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-24000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-23000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-25000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-25000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-24000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-26000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-26000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-25000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-27000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-27000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-26000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-28000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-28000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-27000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-29000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-29000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-28000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-30000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-30000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-29000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-31000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-31000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-30000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-32000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-32000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-31000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-33000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-33000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-32000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-34000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-34000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-33000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-35000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-35000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-35000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-34000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-36000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-36000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-36000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-35000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-37000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-37000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-37000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-36000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-38000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-38000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-38000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-37000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-39000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-39000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-39000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-38000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-40000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-40000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-40000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-39000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-41000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-41000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-41000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-40000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-42000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-42000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-42000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-41000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-43000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-43000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-43000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-42000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-44000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-44000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-44000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-43000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-45000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-45000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-45000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-44000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-46000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-46000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-46000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-45000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-47000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-47000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-47000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-46000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-48000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-48000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-48000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-47000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-49000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-49000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-49000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-48000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-50000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-50000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-50000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-49000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-51000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-51000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-51000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-50000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-52000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-52000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-52000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-51000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-53000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-53000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-53000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-52000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-54000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-54000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-54000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-53000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-55000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-55000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-55000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-54000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-56000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-56000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-56000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-55000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-57000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-57000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-57000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-56000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-58000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-58000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-58000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-57000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-59000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-59000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-59000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-58000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-60000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-60000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-60000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-59000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-61000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-61000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-61000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-60000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-62000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-62000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-62000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-61000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-63000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-63000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-63000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-62000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-64000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-64000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-64000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-63000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-65000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-65000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-65000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-64000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-66000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-66000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-66000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-65000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to: MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling, DataCollatorForWholeWordMask\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import sys\n",
    "import gzip\n",
    "from datetime import datetime\n",
    "\n",
    "model_name = 'emilyalsentzer/Bio_ClinicalBERT'\n",
    "per_device_train_batch_size = 64\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "save_steps = 1000               #Save model every 1k steps\n",
    "num_train_epochs = 7            #Number of epochs\n",
    "use_fp16 = True                #Set to True, if your GPU supports FP16 operations\n",
    "max_length = 100                #Max length for a text input\n",
    "do_whole_word_mask = True       #If set to true, whole words are masked\n",
    "mlm_prob = 0.15                 #Probability that a word is replaced by a [MASK] token\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "output_dir = \"MLMPredtrained/{}-{}\".format(model_name.replace(\"/\", \"_\"),  datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "print(\"Save checkpoints to:\", output_dir)\n",
    "\n",
    "##### Load our training datasets\n",
    "train_path = '/kaggle/input/nbme-preprocessed-sentences/patient_notes_sentences.csv'\n",
    "train_sentences = pd.read_csv(train_path).sentence.tolist()\n",
    "print(\"Train sentences:\", len(train_sentences))\n",
    "\n",
    "dev_sentences = []\n",
    "print(\"Dev sentences:\", len(dev_sentences))\n",
    "\n",
    "#A dataset wrapper, that tokenizes our data on-the-fly\n",
    "class TokenizedSentencesDataset:\n",
    "    def __init__(self, sentences, tokenizer, max_length, cache_tokenization=False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentences = sentences\n",
    "        self.max_length = max_length\n",
    "        self.cache_tokenization = cache_tokenization\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if not self.cache_tokenization:\n",
    "            return self.tokenizer(self.sentences[item], add_special_tokens=True, truncation=True, max_length=self.max_length, return_special_tokens_mask=True)\n",
    "\n",
    "        if isinstance(self.sentences[item], str):\n",
    "            self.sentences[item] = self.tokenizer(self.sentences[item], add_special_tokens=True, truncation=True, max_length=self.max_length, return_special_tokens_mask=True)\n",
    "        return self.sentences[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "train_dataset = TokenizedSentencesDataset(train_sentences, tokenizer, max_length)\n",
    "dev_dataset = TokenizedSentencesDataset(dev_sentences, tokenizer, max_length, cache_tokenization=True) if len(dev_sentences) > 0 else None\n",
    "\n",
    "\n",
    "##### Training arguments\n",
    "if do_whole_word_mask:\n",
    "    data_collator = DataCollatorForWholeWordMask(tokenizer=tokenizer, mlm=True, mlm_probability=mlm_prob)\n",
    "else:\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=mlm_prob)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    evaluation_strategy=\"steps\" if dev_dataset is not None else \"no\",\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    eval_steps=save_steps,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=save_steps,\n",
    "    save_total_limit=1,\n",
    "    prediction_loss_only=True,\n",
    "    fp16=use_fp16\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset\n",
    ")\n",
    "\n",
    "print(\"Save tokenizer to:\", output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"Save model to:\", output_dir)\n",
    "model.save_pretrained(output_dir)\n",
    "\n",
    "print(\"Training done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffa6bc3",
   "metadata": {
    "papermill": {
     "duration": 0.105416,
     "end_time": "2022-04-10T15:22:04.526771",
     "exception": false,
     "start_time": "2022-04-10T15:22:04.421355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3652a22d",
   "metadata": {
    "papermill": {
     "duration": 0.105166,
     "end_time": "2022-04-10T15:22:04.737541",
     "exception": false,
     "start_time": "2022-04-10T15:22:04.632375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 31103.709321,
   "end_time": "2022-04-10T15:22:07.678521",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-10T06:43:43.969200",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "079c6b860a2c45d1931b81b186f9c405": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0a46be5948df4e97847f1ef478eb708b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d2058f96983b468c9a8a5d95f4eedb6f",
        "IPY_MODEL_78495b75cfef4218a2f7c5aecb12d61b",
        "IPY_MODEL_d0886cc3d470415a923a362886a94143"
       ],
       "layout": "IPY_MODEL_6b3400be474b4859a36a95c0bec1de97"
      }
     },
     "0b8a4432424d4a43a2be6b4fd197d0e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_59df7919db5a425d9f899232eed9d1bd",
       "max": 213450.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_fd8b08ab289d48cfb06db0a14b4c7235",
       "value": 213450.0
      }
     },
     "0f9609cb630d4de984f8f08609517901": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1c2afbdb6cb04fe5996f14265a691659": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2a14c7b9968c4c848ab5da7a9337ec2e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2a62ab4b5dc1459b908c6b291c485796": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_cb538a0de0704d8ab4f170a139fc288e",
       "placeholder": "",
       "style": "IPY_MODEL_849ff9b5b7384bb4bc02040718d13a79",
       "value": " 416M/416M [00:14&lt;00:00, 31.3MB/s]"
      }
     },
     "30eefa4c56e54e75bb74a6070bd71f49": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2a14c7b9968c4c848ab5da7a9337ec2e",
       "max": 435778770.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c6ea2511b5124b05ad5a1f7a525e3206",
       "value": 435778770.0
      }
     },
     "400a2cf0666d477083df3503fe1f6f7c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "59df7919db5a425d9f899232eed9d1bd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6b3400be474b4859a36a95c0bec1de97": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7100be32e80e4dd49a4826aa71b439e8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7678a0d7d19841d7ba7258f4533c713e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8c982878922b449584c447fe0eafdbfe",
        "IPY_MODEL_0b8a4432424d4a43a2be6b4fd197d0e4",
        "IPY_MODEL_f85459d724d44e0182d4104642cfc6ff"
       ],
       "layout": "IPY_MODEL_bffa88e33d0946c0974a6cfedc1ad1f7"
      }
     },
     "78495b75cfef4218a2f7c5aecb12d61b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_86df33a387e24f8fb8a33326dfc9c58e",
       "max": 385.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_919db05d8f84490cbbb6bb264421e3c1",
       "value": 385.0
      }
     },
     "849ff9b5b7384bb4bc02040718d13a79": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "86df33a387e24f8fb8a33326dfc9c58e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8c982878922b449584c447fe0eafdbfe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7100be32e80e4dd49a4826aa71b439e8",
       "placeholder": "",
       "style": "IPY_MODEL_0f9609cb630d4de984f8f08609517901",
       "value": "Downloading: 100%"
      }
     },
     "919db05d8f84490cbbb6bb264421e3c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a413455c0c114286a06ef1ce7fd4446b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b2777d08e25748439fd7d0a60397c15a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b67136d08ad24b36a11c5864956fd2bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b884c43ae6504f86810a08aaf95f30e5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "bffa88e33d0946c0974a6cfedc1ad1f7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c68b5628b0bd4d549e0907fccf3a07a3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_079c6b860a2c45d1931b81b186f9c405",
       "placeholder": "",
       "style": "IPY_MODEL_f7ddb8daf0954a5c8f2454dd3defb85f",
       "value": "Downloading: 100%"
      }
     },
     "c6ea2511b5124b05ad5a1f7a525e3206": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "cb538a0de0704d8ab4f170a139fc288e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d0886cc3d470415a923a362886a94143": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e7a3c27982ec4c75ad9ca273812912a9",
       "placeholder": "",
       "style": "IPY_MODEL_a413455c0c114286a06ef1ce7fd4446b",
       "value": " 385/385 [00:00&lt;00:00, 14.9kB/s]"
      }
     },
     "d2058f96983b468c9a8a5d95f4eedb6f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1c2afbdb6cb04fe5996f14265a691659",
       "placeholder": "",
       "style": "IPY_MODEL_b884c43ae6504f86810a08aaf95f30e5",
       "value": "Downloading: 100%"
      }
     },
     "d2dc3a44945248a0ada4bfe96873b9f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c68b5628b0bd4d549e0907fccf3a07a3",
        "IPY_MODEL_30eefa4c56e54e75bb74a6070bd71f49",
        "IPY_MODEL_2a62ab4b5dc1459b908c6b291c485796"
       ],
       "layout": "IPY_MODEL_b2777d08e25748439fd7d0a60397c15a"
      }
     },
     "e7a3c27982ec4c75ad9ca273812912a9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f7ddb8daf0954a5c8f2454dd3defb85f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f85459d724d44e0182d4104642cfc6ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_400a2cf0666d477083df3503fe1f6f7c",
       "placeholder": "",
       "style": "IPY_MODEL_b67136d08ad24b36a11c5864956fd2bb",
       "value": " 208k/208k [00:00&lt;00:00, 540kB/s]"
      }
     },
     "fd8b08ab289d48cfb06db0a14b4c7235": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
