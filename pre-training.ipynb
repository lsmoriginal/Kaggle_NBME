{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b2402ac",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-04-30T09:09:47.972532Z",
     "iopub.status.busy": "2022-04-30T09:09:47.971515Z",
     "iopub.status.idle": "2022-04-30T09:09:49.441853Z",
     "shell.execute_reply": "2022-04-30T09:09:49.441247Z",
     "shell.execute_reply.started": "2022-04-30T08:56:15.534519Z"
    },
    "papermill": {
     "duration": 1.515686,
     "end_time": "2022-04-30T09:09:49.441998",
     "exception": false,
     "start_time": "2022-04-30T09:09:47.926312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nbme-preprocessed-sentences/patient_notes_sentences.csv\n",
      "/kaggle/input/nbme-preprocessed-sentences/data_train_sentence.csv\n",
      "/kaggle/input/nbme-preprocessed-sentences/models/emilyalsentzer_Bio_ClinicalBERT/config.json\n",
      "/kaggle/input/nbme-preprocessed-sentences/models/emilyalsentzer_Bio_ClinicalBERT/README.md\n",
      "/kaggle/input/nbme-preprocessed-sentences/models/emilyalsentzer_Bio_ClinicalBERT/tokenizer.json\n",
      "/kaggle/input/nbme-preprocessed-sentences/models/emilyalsentzer_Bio_ClinicalBERT/tokenizer_config.json\n",
      "/kaggle/input/nbme-preprocessed-sentences/models/emilyalsentzer_Bio_ClinicalBERT/sentence_bert_config.json\n",
      "/kaggle/input/nbme-preprocessed-sentences/models/emilyalsentzer_Bio_ClinicalBERT/pytorch_model.bin\n",
      "/kaggle/input/nbme-preprocessed-sentences/models/emilyalsentzer_Bio_ClinicalBERT/config_sentence_transformers.json\n",
      "/kaggle/input/nbme-preprocessed-sentences/models/emilyalsentzer_Bio_ClinicalBERT/modules.json\n",
      "/kaggle/input/nbme-preprocessed-sentences/models/emilyalsentzer_Bio_ClinicalBERT/special_tokens_map.json\n",
      "/kaggle/input/nbme-preprocessed-sentences/models/emilyalsentzer_Bio_ClinicalBERT/vocab.txt\n",
      "/kaggle/input/nbme-preprocessed-sentences/models/emilyalsentzer_Bio_ClinicalBERT/1_Pooling/config.json\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/config.json\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/tokenizer.json\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/tokenizer_config.json\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/pytorch_model.bin\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/special_tokens_map.json\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/vocab.txt\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/added_tokens.json\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/runs/Apr11_04-12-45_db84cd914c62/events.out.tfevents.1649650370.db84cd914c62.25.0\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/runs/Apr11_04-12-45_db84cd914c62/1649650370.069038/events.out.tfevents.1649650370.db84cd914c62.25.1\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-66000/config.json\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-66000/trainer_state.json\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-66000/training_args.bin\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-66000/pytorch_model.bin\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-66000/scaler.pt\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-66000/scheduler.pt\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-66000/optimizer.pt\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-66000/rng_state.pth\n",
      "/kaggle/input/nbme-score-clinical-patient-notes/sample_submission.csv\n",
      "/kaggle/input/nbme-score-clinical-patient-notes/patient_notes.csv\n",
      "/kaggle/input/nbme-score-clinical-patient-notes/features.csv\n",
      "/kaggle/input/nbme-score-clinical-patient-notes/train.csv\n",
      "/kaggle/input/nbme-score-clinical-patient-notes/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "import nltk\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f760c735",
   "metadata": {
    "papermill": {
     "duration": 0.015837,
     "end_time": "2022-04-30T09:09:49.476377",
     "exception": false,
     "start_time": "2022-04-30T09:09:49.460540",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MLM Pre-training\n",
    "Credit to : [https://github.com/UKPLab/sentence-transformers/blob/master/examples/unsupervised_learning/MLM/train_mlm.py](http://)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a041f534",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T09:09:49.521203Z",
     "iopub.status.busy": "2022-04-30T09:09:49.520345Z",
     "iopub.status.idle": "2022-04-30T09:10:09.681437Z",
     "shell.execute_reply": "2022-04-30T09:10:09.680939Z",
     "shell.execute_reply.started": "2022-04-30T08:57:11.423338Z"
    },
    "papermill": {
     "duration": 20.18902,
     "end_time": "2022-04-30T09:10:09.681582",
     "exception": false,
     "start_time": "2022-04-30T09:09:49.492562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling, DataCollatorForWholeWordMask\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import sys\n",
    "import gzip\n",
    "from datetime import datetime\n",
    "\n",
    "# model_name = 'emilyalsentzer/Bio_ClinicalBERT'\n",
    "# model_name = '../input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23'\n",
    "model_name = '../input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44'\n",
    "model_alias = 'emilyalsentzer_Bio_ClinicalBERT_round3'\n",
    "\n",
    "per_device_train_batch_size = 64\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "save_steps = 1000               #Save model every 1k steps\n",
    "num_train_epochs = 7            #Number of epochs\n",
    "use_fp16 = True                #Set to True, if your GPU supports FP16 operations\n",
    "max_length = 100                #Max length for a text input\n",
    "do_whole_word_mask = True       #If set to true, whole words are masked\n",
    "mlm_prob = 0.15                 #Probability that a word is replaced by a [MASK] token\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73f43fc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T09:10:09.718493Z",
     "iopub.status.busy": "2022-04-30T09:10:09.717763Z",
     "iopub.status.idle": "2022-04-30T09:10:10.391110Z",
     "shell.execute_reply": "2022-04-30T09:10:10.390659Z",
     "shell.execute_reply.started": "2022-04-30T09:06:47.094620Z"
    },
    "papermill": {
     "duration": 0.693304,
     "end_time": "2022-04-30T09:10:10.391264",
     "exception": false,
     "start_time": "2022-04-30T09:10:09.697960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! mkdir MLMPredtrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91351ee2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T09:10:10.426813Z",
     "iopub.status.busy": "2022-04-30T09:10:10.425986Z",
     "iopub.status.idle": "2022-04-30T09:10:21.268044Z",
     "shell.execute_reply": "2022-04-30T09:10:21.267231Z",
     "shell.execute_reply.started": "2022-04-30T09:07:34.233975Z"
    },
    "papermill": {
     "duration": 10.860711,
     "end_time": "2022-04-30T09:10:21.268220",
     "exception": false,
     "start_time": "2022-04-30T09:10:10.407509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a copy\n",
    "! cp -r $model_name \"./MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d3a7488",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T09:10:22.996106Z",
     "iopub.status.busy": "2022-04-30T09:10:22.995366Z",
     "iopub.status.idle": "2022-04-30T09:10:29.808992Z",
     "shell.execute_reply": "2022-04-30T09:10:29.809555Z",
     "shell.execute_reply.started": "2022-04-30T08:57:34.503707Z"
    },
    "papermill": {
     "duration": 8.52506,
     "end_time": "2022-04-30T09:10:29.809766",
     "exception": false,
     "start_time": "2022-04-30T09:10:21.284706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb913feb44c741299f511e51d26e5538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0281e45a2fbf45bd9de2e42b6e46555f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/606664 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# expanding the vocab\n",
    "sentence = pd.read_csv('/kaggle/input/nbme-preprocessed-sentences/patient_notes_sentences.csv')\n",
    "sentence.head()\n",
    "\n",
    "tqdm().pandas()\n",
    "sentence['tokens'] = sentence.sentence.progress_apply(\n",
    "    lambda sent: re.split('\\W', sent)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58449a49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T09:10:30.103006Z",
     "iopub.status.busy": "2022-04-30T09:10:30.102197Z",
     "iopub.status.idle": "2022-04-30T09:10:34.468294Z",
     "shell.execute_reply": "2022-04-30T09:10:34.467333Z",
     "shell.execute_reply.started": "2022-04-30T08:57:41.012261Z"
    },
    "papermill": {
     "duration": 4.632596,
     "end_time": "2022-04-30T09:10:34.468447",
     "exception": false,
     "start_time": "2022-04-30T09:10:29.835851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vocab in the notes\n",
    "allText = \" \".join(sentence.sentence)\n",
    "allTextTokens = re.split('\\W', allText)\n",
    "allTextTokensCounter = Counter(allTextTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "224af1a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T09:10:34.678651Z",
     "iopub.status.busy": "2022-04-30T09:10:34.677526Z",
     "iopub.status.idle": "2022-04-30T09:10:34.699306Z",
     "shell.execute_reply": "2022-04-30T09:10:34.699709Z",
     "shell.execute_reply.started": "2022-04-30T08:57:44.710734Z"
    },
    "papermill": {
     "duration": 0.213631,
     "end_time": "2022-04-30T09:10:34.699847",
     "exception": false,
     "start_time": "2022-04-30T09:10:34.486216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'first element:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# add the top 3000 tokens to the vocabs\n",
    "top3k = list(token for token, freq in allTextTokensCounter.most_common(3000))\n",
    "display('first element:', top3k[0])\n",
    "del top3k[0]\n",
    "# top3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e73c1fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T09:10:34.750045Z",
     "iopub.status.busy": "2022-04-30T09:10:34.749317Z",
     "iopub.status.idle": "2022-04-30T09:10:34.756405Z",
     "shell.execute_reply": "2022-04-30T09:10:34.755951Z",
     "shell.execute_reply.started": "2022-04-30T08:57:44.909868Z"
    },
    "papermill": {
     "duration": 0.038699,
     "end_time": "2022-04-30T09:10:34.756518",
     "exception": false,
     "start_time": "2022-04-30T09:10:34.717819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30358, 768, padding_idx=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newTokensAdded = tokenizer.add_tokens(top3k)\n",
    "display(newTokensAdded)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e5a8f76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T09:10:34.798542Z",
     "iopub.status.busy": "2022-04-30T09:10:34.797858Z",
     "iopub.status.idle": "2022-04-30T09:10:34.800439Z",
     "shell.execute_reply": "2022-04-30T09:10:34.799998Z",
     "shell.execute_reply.started": "2022-04-30T08:57:44.931674Z"
    },
    "papermill": {
     "duration": 0.025368,
     "end_time": "2022-04-30T09:10:34.800545",
     "exception": false,
     "start_time": "2022-04-30T09:10:34.775177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "onlywords = re.compile('^[^\\W\\s\\d]+$')\n",
    "\n",
    "exclusiveWords = {\n",
    "    'and', 'or'\n",
    "}\n",
    "\n",
    "def isValidGram(grams):\n",
    "    return all([all(re.match(onlywords, g) for g in grams),\n",
    "                all(word not in exclusiveWords for words in grams)\n",
    "               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79b2a434",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T09:10:34.846638Z",
     "iopub.status.busy": "2022-04-30T09:10:34.846006Z",
     "iopub.status.idle": "2022-04-30T09:10:34.848909Z",
     "shell.execute_reply": "2022-04-30T09:10:34.849351Z",
     "shell.execute_reply.started": "2022-04-30T08:57:44.940842Z"
    },
    "papermill": {
     "duration": 0.028748,
     "end_time": "2022-04-30T09:10:34.849473",
     "exception": false,
     "start_time": "2022-04-30T09:10:34.820725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nallTextSplitBySpace = re.split('\\\\s', allText)\\nbigrm = nltk.bigrams(allTextSplitBySpace)\\nbigrm = filter(isValidGram, bigrm)\\nbigrmCtr = Counter(bigrm)\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "allTextSplitBySpace = re.split('\\s', allText)\n",
    "bigrm = nltk.bigrams(allTextSplitBySpace)\n",
    "bigrm = filter(isValidGram, bigrm)\n",
    "bigrmCtr = Counter(bigrm)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96bccc5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T09:10:34.893686Z",
     "iopub.status.busy": "2022-04-30T09:10:34.892873Z",
     "iopub.status.idle": "2022-04-30T09:10:34.895439Z",
     "shell.execute_reply": "2022-04-30T09:10:34.894943Z",
     "shell.execute_reply.started": "2022-04-30T08:57:44.953011Z"
    },
    "papermill": {
     "duration": 0.025835,
     "end_time": "2022-04-30T09:10:34.895546",
     "exception": false,
     "start_time": "2022-04-30T09:10:34.869711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# topBgrm = \n",
    "# bigrmCtr.most_common(1000)[-50:]\n",
    "# bigrmCtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45aa1fb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T09:10:34.939455Z",
     "iopub.status.busy": "2022-04-30T09:10:34.938657Z",
     "iopub.status.idle": "2022-04-30T09:10:34.941016Z",
     "shell.execute_reply": "2022-04-30T09:10:34.940590Z",
     "shell.execute_reply.started": "2022-04-30T08:57:44.959259Z"
    },
    "papermill": {
     "duration": 0.025384,
     "end_time": "2022-04-30T09:10:34.941127",
     "exception": false,
     "start_time": "2022-04-30T09:10:34.915743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# topBgrm\n",
    "# for i in range(10):\n",
    "#     print(topBgrm[i][0], isValidGram(topBgrm[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d60348d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T09:10:34.984884Z",
     "iopub.status.busy": "2022-04-30T09:10:34.984110Z",
     "iopub.status.idle": "2022-04-30T09:10:34.986010Z",
     "shell.execute_reply": "2022-04-30T09:10:34.986444Z",
     "shell.execute_reply.started": "2022-04-30T08:57:49.728302Z"
    },
    "papermill": {
     "duration": 0.025712,
     "end_time": "2022-04-30T09:10:34.986558",
     "exception": false,
     "start_time": "2022-04-30T09:10:34.960846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bigrmCtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "170e640f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T09:10:35.029948Z",
     "iopub.status.busy": "2022-04-30T09:10:35.029326Z",
     "iopub.status.idle": "2022-04-30T09:10:35.506691Z",
     "shell.execute_reply": "2022-04-30T09:10:35.507112Z",
     "shell.execute_reply.started": "2022-04-30T08:57:53.008813Z"
    },
    "papermill": {
     "duration": 0.501035,
     "end_time": "2022-04-30T09:10:35.507279",
     "exception": false,
     "start_time": "2022-04-30T09:10:35.006244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30358, 768, padding_idx=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab in the labels\n",
    "data_train = pd.read_csv('/kaggle/input/nbme-preprocessed-sentences/data_train_sentence.csv')\n",
    "featureTokens = data_train.feature_num.apply(str).unique().tolist()\n",
    "\n",
    "newTokensAdded = tokenizer.add_tokens(featureTokens)\n",
    "display(newTokensAdded)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40c93392",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T09:10:35.563025Z",
     "iopub.status.busy": "2022-04-30T09:10:35.562212Z",
     "iopub.status.idle": "2022-04-30T17:40:34.041857Z",
     "shell.execute_reply": "2022-04-30T17:40:34.041378Z",
     "shell.execute_reply.started": "2022-04-30T08:58:25.381362Z"
    },
    "papermill": {
     "duration": 30598.514739,
     "end_time": "2022-04-30T17:40:34.041985",
     "exception": false,
     "start_time": "2022-04-30T09:10:35.527246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save checkpoints to: MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sentences: 606664\n",
      "Dev sentences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n",
      "tokenizer config file saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/tokenizer_config.json\n",
      "Special tokens file saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 606664\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 66360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save tokenizer to: MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35\n",
      "training begins\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66360' max='66360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [66360/66360 8:29:51, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.742900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.724200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.711500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.697700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.679100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.676700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.673400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.662700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.662900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.644600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.639300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.627800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.633800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.630200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.621000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.625800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.625700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.619200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.624400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.611400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.600200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.614100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.607000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.597900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.603200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.610500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.613100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.605800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.601200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.601600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.599400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.598900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.606800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.609200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.605600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.605700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.602800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.606400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.595400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.602600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.602400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.603100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.606400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.609000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.619200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.617500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.623000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.613700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.616800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.623400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.622700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.629200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.624700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.629500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.633400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.642800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>0.637900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.645500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>0.646600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.642900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>0.649300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.659900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>0.664200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.668700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.670300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.670900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-1000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-1000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-2000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-2000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-3000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-3000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-2000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-4000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-4000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-5000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-5000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-4000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-6000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-6000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-7000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-7000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-8000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-8000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-9000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-9000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-8000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-10000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-10000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-9000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-11000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-11000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-12000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-12000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-13000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-13000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-12000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-14000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-14000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-13000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-15000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-15000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-14000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-16000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-16000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-15000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-17000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-17000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-16000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-18000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-18000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-17000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-19000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-19000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-18000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-20000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-20000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-19000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-21000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-21000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-20000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-22000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-22000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-21000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-23000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-23000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-22000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-24000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-24000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-23000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-25000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-25000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-24000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-26000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-26000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-25000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-27000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-27000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-26000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-28000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-28000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-27000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-29000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-29000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-28000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-30000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-30000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-29000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-31000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-31000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-30000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-32000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-32000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-31000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-33000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-33000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-32000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-34000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-34000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-33000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-35000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-35000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-35000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-34000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-36000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-36000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-36000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-35000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-37000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-37000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-37000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-36000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-38000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-38000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-38000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-37000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-39000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-39000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-39000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-38000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-40000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-40000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-40000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-39000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-41000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-41000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-41000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-40000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-42000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-42000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-42000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-41000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-43000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-43000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-43000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-42000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-44000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-44000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-44000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-43000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-45000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-45000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-45000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-44000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-46000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-46000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-46000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-45000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-47000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-47000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-47000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-46000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-48000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-48000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-48000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-47000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-49000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-49000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-49000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-48000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-50000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-50000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-50000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-49000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-51000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-51000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-51000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-50000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-52000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-52000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-52000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-51000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-53000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-53000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-53000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-52000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-54000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-54000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-54000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-53000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-55000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-55000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-55000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-54000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-56000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-56000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-56000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-55000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-57000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-57000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-57000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-56000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-58000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-58000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-58000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-57000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-59000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-59000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-59000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-58000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-60000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-60000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-60000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-59000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-61000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-61000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-61000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-60000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-62000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-62000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-62000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-61000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-63000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-63000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-63000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-62000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-64000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-64000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-64000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-63000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-65000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-65000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-65000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-64000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-66000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-66000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-66000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/checkpoint-65000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to: MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round3-2022-04-30_09-10-35/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"MLMPredtrained/{}-{}\".format(model_alias.replace(\"/\", \"_\"),  datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "print(\"Save checkpoints to:\", output_dir)\n",
    "\n",
    "##### Load our training datasets\n",
    "train_path = '/kaggle/input/nbme-preprocessed-sentences/patient_notes_sentences.csv'\n",
    "train_sentences = pd.read_csv(train_path).sentence.tolist()\n",
    "print(\"Train sentences:\", len(train_sentences))\n",
    "\n",
    "dev_sentences = []\n",
    "print(\"Dev sentences:\", len(dev_sentences))\n",
    "\n",
    "#A dataset wrapper, that tokenizes our data on-the-fly\n",
    "class TokenizedSentencesDataset:\n",
    "    def __init__(self, sentences, tokenizer, max_length, cache_tokenization=False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentences = sentences\n",
    "        self.max_length = max_length\n",
    "        self.cache_tokenization = cache_tokenization\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if not self.cache_tokenization:\n",
    "            return self.tokenizer(self.sentences[item], add_special_tokens=True, truncation=True, max_length=self.max_length, return_special_tokens_mask=True)\n",
    "\n",
    "        if isinstance(self.sentences[item], str):\n",
    "            self.sentences[item] = self.tokenizer(self.sentences[item], add_special_tokens=True, truncation=True, max_length=self.max_length, return_special_tokens_mask=True)\n",
    "        return self.sentences[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "train_dataset = TokenizedSentencesDataset(train_sentences, tokenizer, max_length)\n",
    "dev_dataset = TokenizedSentencesDataset(dev_sentences, tokenizer, max_length, cache_tokenization=True) if len(dev_sentences) > 0 else None\n",
    "\n",
    "\n",
    "##### Training arguments\n",
    "if do_whole_word_mask:\n",
    "    data_collator = DataCollatorForWholeWordMask(tokenizer=tokenizer, mlm=True, mlm_probability=mlm_prob)\n",
    "else:\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=mlm_prob)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    evaluation_strategy=\"steps\" if dev_dataset is not None else \"no\",\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    eval_steps=save_steps,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=save_steps,\n",
    "    save_total_limit=1,\n",
    "    prediction_loss_only=True,\n",
    "    fp16=use_fp16\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset\n",
    ")\n",
    "\n",
    "print(\"Save tokenizer to:\", output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print('training begins')\n",
    "trainer.train()\n",
    "\n",
    "print(\"Save model to:\", output_dir)\n",
    "model.save_pretrained(output_dir)\n",
    "\n",
    "print(\"Training done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65efc1d9",
   "metadata": {
    "papermill": {
     "duration": 0.112028,
     "end_time": "2022-04-30T17:40:34.266577",
     "exception": false,
     "start_time": "2022-04-30T17:40:34.154549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b9b8cb",
   "metadata": {
    "papermill": {
     "duration": 0.112142,
     "end_time": "2022-04-30T17:40:34.492791",
     "exception": false,
     "start_time": "2022-04-30T17:40:34.380649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654bf129",
   "metadata": {
    "papermill": {
     "duration": 0.111945,
     "end_time": "2022-04-30T17:40:34.718625",
     "exception": false,
     "start_time": "2022-04-30T17:40:34.606680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30658.465472,
   "end_time": "2022-04-30T17:40:38.287115",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-30T09:09:39.821643",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01bb9d84dc014d45aff891d32c2f20b9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "01ed017abacb4c6abfac2cdba3e0d032": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "0281e45a2fbf45bd9de2e42b6e46555f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ab4486233636434184b6576fef9f0378",
        "IPY_MODEL_f4c645bcd2d94ca194935216cb6804ae",
        "IPY_MODEL_b36a413b3d81450887f1d7e3c5e08036"
       ],
       "layout": "IPY_MODEL_31c65e739c044045aee1ea668649d2df"
      }
     },
     "0aeb84c8bf4f4d37970627831e95250d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "10063b1844434850be6ab76b098fdf09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "148285234a4941cdaffee2ace04b3a88": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "153b87f3c8414a91a59a8875c6dbce97": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1d821267a484463fb17e2c754b060686": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "25e9f1ea2e9143bd9622c247fe84a3aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_38dc1140e9c24f1dba46eda46f5df5f4",
       "placeholder": "​",
       "style": "IPY_MODEL_148285234a4941cdaffee2ace04b3a88",
       "value": ""
      }
     },
     "2679523ab4284d0aae5947c17d5d53c9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_153b87f3c8414a91a59a8875c6dbce97",
       "placeholder": "​",
       "style": "IPY_MODEL_b4282e2ff850477fa238ffb8f802e8f3",
       "value": " 0/? [00:00&lt;?, ?it/s]"
      }
     },
     "31c65e739c044045aee1ea668649d2df": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "355dd380ef094df0802d2839125abebd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "38dc1140e9c24f1dba46eda46f5df5f4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "454006c50411455581bd4f8364d3c341": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "963b4814562141b48313d08fe3d49238": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ab4486233636434184b6576fef9f0378": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_454006c50411455581bd4f8364d3c341",
       "placeholder": "​",
       "style": "IPY_MODEL_10063b1844434850be6ab76b098fdf09",
       "value": "100%"
      }
     },
     "b36a413b3d81450887f1d7e3c5e08036": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_be28c46ddc01455aa4bc6fbf1535ef75",
       "placeholder": "​",
       "style": "IPY_MODEL_01ed017abacb4c6abfac2cdba3e0d032",
       "value": " 606664/606664 [00:05&lt;00:00, 124422.22it/s]"
      }
     },
     "b4282e2ff850477fa238ffb8f802e8f3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "be28c46ddc01455aa4bc6fbf1535ef75": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eb913feb44c741299f511e51d26e5538": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_25e9f1ea2e9143bd9622c247fe84a3aa",
        "IPY_MODEL_f7b5a0b4e7054d2696a3e28bf9fd8815",
        "IPY_MODEL_2679523ab4284d0aae5947c17d5d53c9"
       ],
       "layout": "IPY_MODEL_01bb9d84dc014d45aff891d32c2f20b9"
      }
     },
     "f4c645bcd2d94ca194935216cb6804ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1d821267a484463fb17e2c754b060686",
       "max": 606664.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_355dd380ef094df0802d2839125abebd",
       "value": 606664.0
      }
     },
     "f7b5a0b4e7054d2696a3e28bf9fd8815": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0aeb84c8bf4f4d37970627831e95250d",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_963b4814562141b48313d08fe3d49238",
       "value": 0.0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
