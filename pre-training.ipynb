{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bec37208",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-04-11T04:12:19.412690Z",
     "iopub.status.busy": "2022-04-11T04:12:19.408736Z",
     "iopub.status.idle": "2022-04-11T04:12:20.766683Z",
     "shell.execute_reply": "2022-04-11T04:12:20.767178Z",
     "shell.execute_reply.started": "2022-04-11T04:10:53.165663Z"
    },
    "papermill": {
     "duration": 1.384495,
     "end_time": "2022-04-11T04:12:20.767475",
     "exception": false,
     "start_time": "2022-04-11T04:12:19.382980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nbme-preprocessed-sentences/patient_notes_sentences.csv\n",
      "/kaggle/input/nbme-preprocessed-sentences/data_train_sentence.csv\n",
      "/kaggle/input/nbme-preprocessed-sentences/models/emilyalsentzer_Bio_ClinicalBERT/config.json\n",
      "/kaggle/input/nbme-preprocessed-sentences/models/emilyalsentzer_Bio_ClinicalBERT/README.md\n",
      "/kaggle/input/nbme-preprocessed-sentences/models/emilyalsentzer_Bio_ClinicalBERT/tokenizer.json\n",
      "/kaggle/input/nbme-preprocessed-sentences/models/emilyalsentzer_Bio_ClinicalBERT/tokenizer_config.json\n",
      "/kaggle/input/nbme-preprocessed-sentences/models/emilyalsentzer_Bio_ClinicalBERT/sentence_bert_config.json\n",
      "/kaggle/input/nbme-preprocessed-sentences/models/emilyalsentzer_Bio_ClinicalBERT/pytorch_model.bin\n",
      "/kaggle/input/nbme-preprocessed-sentences/models/emilyalsentzer_Bio_ClinicalBERT/config_sentence_transformers.json\n",
      "/kaggle/input/nbme-preprocessed-sentences/models/emilyalsentzer_Bio_ClinicalBERT/modules.json\n",
      "/kaggle/input/nbme-preprocessed-sentences/models/emilyalsentzer_Bio_ClinicalBERT/special_tokens_map.json\n",
      "/kaggle/input/nbme-preprocessed-sentences/models/emilyalsentzer_Bio_ClinicalBERT/vocab.txt\n",
      "/kaggle/input/nbme-preprocessed-sentences/models/emilyalsentzer_Bio_ClinicalBERT/1_Pooling/config.json\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/config.json\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/tokenizer.json\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/tokenizer_config.json\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/pytorch_model.bin\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/special_tokens_map.json\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/vocab.txt\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/runs/Apr10_06-44-23_2a2a7c3235a4/events.out.tfevents.1649573068.2a2a7c3235a4.23.0\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/runs/Apr10_06-44-23_2a2a7c3235a4/1649573068.919302/events.out.tfevents.1649573068.2a2a7c3235a4.23.1\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-66000/config.json\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-66000/trainer_state.json\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-66000/training_args.bin\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-66000/pytorch_model.bin\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-66000/scaler.pt\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-66000/scheduler.pt\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-66000/optimizer.pt\n",
      "/kaggle/input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23/checkpoint-66000/rng_state.pth\n",
      "/kaggle/input/nbme-score-clinical-patient-notes/sample_submission.csv\n",
      "/kaggle/input/nbme-score-clinical-patient-notes/patient_notes.csv\n",
      "/kaggle/input/nbme-score-clinical-patient-notes/features.csv\n",
      "/kaggle/input/nbme-score-clinical-patient-notes/train.csv\n",
      "/kaggle/input/nbme-score-clinical-patient-notes/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "import nltk\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e86566c",
   "metadata": {
    "papermill": {
     "duration": 0.015497,
     "end_time": "2022-04-11T04:12:20.801026",
     "exception": false,
     "start_time": "2022-04-11T04:12:20.785529",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MLM Pre-training\n",
    "Credit to : [https://github.com/UKPLab/sentence-transformers/blob/master/examples/unsupervised_learning/MLM/train_mlm.py](http://)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e034c0a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T04:12:20.838940Z",
     "iopub.status.busy": "2022-04-11T04:12:20.838244Z",
     "iopub.status.idle": "2022-04-11T04:12:31.952846Z",
     "shell.execute_reply": "2022-04-11T04:12:31.955432Z",
     "shell.execute_reply.started": "2022-04-11T04:10:53.318932Z"
    },
    "papermill": {
     "duration": 11.139017,
     "end_time": "2022-04-11T04:12:31.955666",
     "exception": false,
     "start_time": "2022-04-11T04:12:20.816649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling, DataCollatorForWholeWordMask\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import sys\n",
    "import gzip\n",
    "from datetime import datetime\n",
    "\n",
    "# model_name = 'emilyalsentzer/Bio_ClinicalBERT'\n",
    "model_name = '../input/nbme-mlm-pretrained/MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT-2022-04-10_06-44-23'\n",
    "model_alias = 'emilyalsentzer_Bio_ClinicalBERT_round2'\n",
    "\n",
    "per_device_train_batch_size = 64\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "save_steps = 1000               #Save model every 1k steps\n",
    "num_train_epochs = 7            #Number of epochs\n",
    "use_fp16 = True                #Set to True, if your GPU supports FP16 operations\n",
    "max_length = 100                #Max length for a text input\n",
    "do_whole_word_mask = True       #If set to true, whole words are masked\n",
    "mlm_prob = 0.15                 #Probability that a word is replaced by a [MASK] token\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80b23dc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T04:12:32.024056Z",
     "iopub.status.busy": "2022-04-11T04:12:32.023090Z",
     "iopub.status.idle": "2022-04-11T04:12:38.563366Z",
     "shell.execute_reply": "2022-04-11T04:12:38.562920Z",
     "shell.execute_reply.started": "2022-04-11T04:10:54.849871Z"
    },
    "papermill": {
     "duration": 6.581224,
     "end_time": "2022-04-11T04:12:38.563495",
     "exception": false,
     "start_time": "2022-04-11T04:12:31.982271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3425b1d0e06c41e285c489f49cb94867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97dac9a54524a74b288af347e4be224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/606664 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# expanding the vocab\n",
    "sentence = pd.read_csv('/kaggle/input/nbme-preprocessed-sentences/patient_notes_sentences.csv')\n",
    "sentence.head()\n",
    "\n",
    "tqdm().pandas()\n",
    "sentence['tokens'] = sentence.sentence.progress_apply(\n",
    "    lambda sent: re.split('\\W', sent)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "715451ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T04:12:42.434414Z",
     "iopub.status.busy": "2022-04-11T04:12:42.433636Z",
     "iopub.status.idle": "2022-04-11T04:12:42.436523Z",
     "shell.execute_reply": "2022-04-11T04:12:42.436058Z",
     "shell.execute_reply.started": "2022-04-11T04:11:02.036365Z"
    },
    "papermill": {
     "duration": 3.85553,
     "end_time": "2022-04-11T04:12:42.436657",
     "exception": false,
     "start_time": "2022-04-11T04:12:38.581127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vocab in the notes\n",
    "allText = \" \".join(sentence.sentence)\n",
    "allTextTokens = re.split('\\W', allText)\n",
    "allTextTokensCounter = Counter(allTextTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52dbdf95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T04:12:42.668829Z",
     "iopub.status.busy": "2022-04-11T04:12:42.647349Z",
     "iopub.status.idle": "2022-04-11T04:12:42.674112Z",
     "shell.execute_reply": "2022-04-11T04:12:42.673714Z",
     "shell.execute_reply.started": "2022-04-11T04:11:05.720070Z"
    },
    "papermill": {
     "duration": 0.220216,
     "end_time": "2022-04-11T04:12:42.674231",
     "exception": false,
     "start_time": "2022-04-11T04:12:42.454015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'first element:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# add the top 3000 tokens to the vocabs\n",
    "top3k = list(token for token, freq in allTextTokensCounter.most_common(3000))\n",
    "display('first element:', top3k[0])\n",
    "del top3k[0]\n",
    "# top3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beb06e3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T04:12:42.726874Z",
     "iopub.status.busy": "2022-04-11T04:12:42.726302Z",
     "iopub.status.idle": "2022-04-11T04:12:43.278924Z",
     "shell.execute_reply": "2022-04-11T04:12:43.280034Z",
     "shell.execute_reply.started": "2022-04-11T04:11:05.997526Z"
    },
    "papermill": {
     "duration": 0.588617,
     "end_time": "2022-04-11T04:12:43.280291",
     "exception": false,
     "start_time": "2022-04-11T04:12:42.691674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1291"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30287, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newTokensAdded = tokenizer.add_tokens(top3k)\n",
    "display(newTokensAdded)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbeae495",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T04:12:43.346433Z",
     "iopub.status.busy": "2022-04-11T04:12:43.345638Z",
     "iopub.status.idle": "2022-04-11T04:12:43.349625Z",
     "shell.execute_reply": "2022-04-11T04:12:43.350320Z",
     "shell.execute_reply.started": "2022-04-11T04:11:06.673402Z"
    },
    "papermill": {
     "duration": 0.040793,
     "end_time": "2022-04-11T04:12:43.350496",
     "exception": false,
     "start_time": "2022-04-11T04:12:43.309703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "onlywords = re.compile('^[^\\W\\s\\d]+$')\n",
    "\n",
    "exclusiveWords = {\n",
    "    'and', 'or'\n",
    "}\n",
    "\n",
    "def isValidGram(grams):\n",
    "    return all([all(re.match(onlywords, g) for g in grams),\n",
    "                all(word not in exclusiveWords for words in grams)\n",
    "               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9dea92f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T04:12:43.420881Z",
     "iopub.status.busy": "2022-04-11T04:12:43.419963Z",
     "iopub.status.idle": "2022-04-11T04:12:43.423525Z",
     "shell.execute_reply": "2022-04-11T04:12:43.421593Z",
     "shell.execute_reply.started": "2022-04-11T04:11:06.686665Z"
    },
    "papermill": {
     "duration": 0.042243,
     "end_time": "2022-04-11T04:12:43.423685",
     "exception": false,
     "start_time": "2022-04-11T04:12:43.381442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nallTextSplitBySpace = re.split('\\\\s', allText)\\nbigrm = nltk.bigrams(allTextSplitBySpace)\\nbigrm = filter(isValidGram, bigrm)\\nbigrmCtr = Counter(bigrm)\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "allTextSplitBySpace = re.split('\\s', allText)\n",
    "bigrm = nltk.bigrams(allTextSplitBySpace)\n",
    "bigrm = filter(isValidGram, bigrm)\n",
    "bigrmCtr = Counter(bigrm)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa499df6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T04:12:43.492078Z",
     "iopub.status.busy": "2022-04-11T04:12:43.491195Z",
     "iopub.status.idle": "2022-04-11T04:12:43.493538Z",
     "shell.execute_reply": "2022-04-11T04:12:43.492832Z",
     "shell.execute_reply.started": "2022-04-11T04:11:06.698525Z"
    },
    "papermill": {
     "duration": 0.038695,
     "end_time": "2022-04-11T04:12:43.493691",
     "exception": false,
     "start_time": "2022-04-11T04:12:43.454996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# topBgrm = \n",
    "# bigrmCtr.most_common(1000)[-50:]\n",
    "# bigrmCtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cd03d2",
   "metadata": {
    "papermill": {
     "duration": 0.024956,
     "end_time": "2022-04-11T04:12:43.551584",
     "exception": false,
     "start_time": "2022-04-11T04:12:43.526628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ac94dc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T04:12:43.595481Z",
     "iopub.status.busy": "2022-04-11T04:12:43.594712Z",
     "iopub.status.idle": "2022-04-11T04:12:43.597820Z",
     "shell.execute_reply": "2022-04-11T04:12:43.597392Z",
     "shell.execute_reply.started": "2022-04-11T04:11:06.709745Z"
    },
    "papermill": {
     "duration": 0.026562,
     "end_time": "2022-04-11T04:12:43.597931",
     "exception": false,
     "start_time": "2022-04-11T04:12:43.571369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# topBgrm\n",
    "# for i in range(10):\n",
    "#     print(topBgrm[i][0], isValidGram(topBgrm[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c9f58cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T04:12:43.641072Z",
     "iopub.status.busy": "2022-04-11T04:12:43.639377Z",
     "iopub.status.idle": "2022-04-11T04:12:43.641715Z",
     "shell.execute_reply": "2022-04-11T04:12:43.642111Z",
     "shell.execute_reply.started": "2022-04-11T04:11:06.720743Z"
    },
    "papermill": {
     "duration": 0.025366,
     "end_time": "2022-04-11T04:12:43.642224",
     "exception": false,
     "start_time": "2022-04-11T04:12:43.616858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bigrmCtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a76fc8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T04:12:43.684117Z",
     "iopub.status.busy": "2022-04-11T04:12:43.683625Z",
     "iopub.status.idle": "2022-04-11T04:12:44.595933Z",
     "shell.execute_reply": "2022-04-11T04:12:44.597494Z",
     "shell.execute_reply.started": "2022-04-11T04:11:06.728959Z"
    },
    "papermill": {
     "duration": 0.937191,
     "end_time": "2022-04-11T04:12:44.598277",
     "exception": false,
     "start_time": "2022-04-11T04:12:43.661086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30358, 768)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab in the labels\n",
    "data_train = pd.read_csv('/kaggle/input/nbme-preprocessed-sentences/data_train_sentence.csv')\n",
    "featureTokens = data_train.feature_num.apply(str).unique().tolist()\n",
    "\n",
    "newTokensAdded = tokenizer.add_tokens(featureTokens)\n",
    "display(newTokensAdded)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09bce858",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T04:12:44.698719Z",
     "iopub.status.busy": "2022-04-11T04:12:44.697421Z",
     "iopub.status.idle": "2022-04-11T12:43:10.886168Z",
     "shell.execute_reply": "2022-04-11T12:43:10.886638Z",
     "shell.execute_reply.started": "2022-04-11T04:11:07.408904Z"
    },
    "papermill": {
     "duration": 30626.246332,
     "end_time": "2022-04-11T12:43:10.886816",
     "exception": false,
     "start_time": "2022-04-11T04:12:44.640484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save checkpoints to: MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sentences: 606664\n",
      "Dev sentences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n",
      "tokenizer config file saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/tokenizer_config.json\n",
      "Special tokens file saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 606664\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 66360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save tokenizer to: MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66360' max='66360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [66360/66360 8:30:18, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.854200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.597600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.324900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.195400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.121300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.081000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.051700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.014500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.006500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.970900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.959500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.933000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.934900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.925700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.900800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.898700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.898500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.881900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.887900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.863000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.847800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.854400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.844900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.828400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.831500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.833600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.833700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.817700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.813000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.806400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.798800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.795300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.800100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.796300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.789500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.784100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.776100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.773700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.762100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.765400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.762200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.756000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.755300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.754300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.756200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.753500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.755100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.740500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.738900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.740700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.733900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.734200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.728400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.726100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.724000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.727500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>0.716200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.723800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>0.715900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.706100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>0.710200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.712900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>0.713700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.711700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.706500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.701800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-1000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-1000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-2000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-2000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-3000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-3000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-4000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-4000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-3000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-5000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-5000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-6000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-6000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-7000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-7000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-6000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-8000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-8000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-7000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-9000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-9000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-10000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-10000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-11000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-11000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-10000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-12000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-12000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-13000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-13000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-12000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-14000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-14000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-13000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-15000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-15000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-14000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-16000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-16000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-15000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-17000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-17000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-16000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-18000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-18000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-17000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-19000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-19000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-18000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-20000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-20000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-19000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-21000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-21000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-20000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-22000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-22000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-21000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-23000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-23000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-22000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-24000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-24000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-23000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-25000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-25000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-24000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-26000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-26000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-25000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-27000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-27000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-26000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-28000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-28000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-27000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-29000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-29000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-28000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-30000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-30000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-29000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-31000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-31000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-30000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-32000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-32000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-31000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-33000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-33000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-32000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-34000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-34000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-33000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-35000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-35000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-35000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-34000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-36000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-36000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-36000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-35000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-37000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-37000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-37000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-36000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-38000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-38000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-38000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-37000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-39000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-39000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-39000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-38000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-40000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-40000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-40000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-39000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-41000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-41000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-41000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-40000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-42000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-42000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-42000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-41000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-43000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-43000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-43000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-42000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-44000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-44000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-44000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-43000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-45000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-45000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-45000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-44000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-46000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-46000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-46000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-45000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-47000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-47000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-47000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-46000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-48000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-48000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-48000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-47000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-49000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-49000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-49000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-48000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-50000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-50000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-50000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-49000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-51000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-51000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-51000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-50000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-52000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-52000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-52000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-51000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-53000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-53000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-53000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-52000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-54000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-54000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-54000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-53000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-55000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-55000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-55000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-54000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-56000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-56000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-56000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-55000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-57000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-57000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-57000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-56000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-58000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-58000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-58000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-57000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-59000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-59000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-59000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-58000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-60000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-60000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-60000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-59000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-61000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-61000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-61000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-60000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-62000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-62000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-62000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-61000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-63000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-63000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-63000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-62000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-64000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-64000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-64000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-63000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-65000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-65000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-65000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-64000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-66000\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-66000/config.json\n",
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-66000/pytorch_model.bin\n",
      "Deleting older checkpoint [MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/checkpoint-65000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to: MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in MLMPredtrained/emilyalsentzer_Bio_ClinicalBERT_round2-2022-04-11_04-12-44/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"MLMPredtrained/{}-{}\".format(model_alias.replace(\"/\", \"_\"),  datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "print(\"Save checkpoints to:\", output_dir)\n",
    "\n",
    "##### Load our training datasets\n",
    "train_path = '/kaggle/input/nbme-preprocessed-sentences/patient_notes_sentences.csv'\n",
    "train_sentences = pd.read_csv(train_path).sentence.tolist()\n",
    "print(\"Train sentences:\", len(train_sentences))\n",
    "\n",
    "dev_sentences = []\n",
    "print(\"Dev sentences:\", len(dev_sentences))\n",
    "\n",
    "#A dataset wrapper, that tokenizes our data on-the-fly\n",
    "class TokenizedSentencesDataset:\n",
    "    def __init__(self, sentences, tokenizer, max_length, cache_tokenization=False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentences = sentences\n",
    "        self.max_length = max_length\n",
    "        self.cache_tokenization = cache_tokenization\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if not self.cache_tokenization:\n",
    "            return self.tokenizer(self.sentences[item], add_special_tokens=True, truncation=True, max_length=self.max_length, return_special_tokens_mask=True)\n",
    "\n",
    "        if isinstance(self.sentences[item], str):\n",
    "            self.sentences[item] = self.tokenizer(self.sentences[item], add_special_tokens=True, truncation=True, max_length=self.max_length, return_special_tokens_mask=True)\n",
    "        return self.sentences[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "train_dataset = TokenizedSentencesDataset(train_sentences, tokenizer, max_length)\n",
    "dev_dataset = TokenizedSentencesDataset(dev_sentences, tokenizer, max_length, cache_tokenization=True) if len(dev_sentences) > 0 else None\n",
    "\n",
    "\n",
    "##### Training arguments\n",
    "if do_whole_word_mask:\n",
    "    data_collator = DataCollatorForWholeWordMask(tokenizer=tokenizer, mlm=True, mlm_probability=mlm_prob)\n",
    "else:\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=mlm_prob)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    evaluation_strategy=\"steps\" if dev_dataset is not None else \"no\",\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    eval_steps=save_steps,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=save_steps,\n",
    "    save_total_limit=1,\n",
    "    prediction_loss_only=True,\n",
    "    fp16=use_fp16\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset\n",
    ")\n",
    "\n",
    "print(\"Save tokenizer to:\", output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"Save model to:\", output_dir)\n",
    "model.save_pretrained(output_dir)\n",
    "\n",
    "print(\"Training done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4254f88",
   "metadata": {
    "papermill": {
     "duration": 0.27007,
     "end_time": "2022-04-11T12:43:11.356328",
     "exception": false,
     "start_time": "2022-04-11T12:43:11.086258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e121531",
   "metadata": {
    "papermill": {
     "duration": 0.113671,
     "end_time": "2022-04-11T12:43:11.582309",
     "exception": false,
     "start_time": "2022-04-11T12:43:11.468638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ade923",
   "metadata": {
    "papermill": {
     "duration": 0.114303,
     "end_time": "2022-04-11T12:43:11.811675",
     "exception": false,
     "start_time": "2022-04-11T12:43:11.697372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30663.719544,
   "end_time": "2022-04-11T12:43:14.806616",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-11T04:12:11.087072",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0e5fbc3a2a4842c8bcf846822a21ccfa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "20399f12c86f4bc2b90507750334dbf9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3425b1d0e06c41e285c489f49cb94867": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ec265fce31414295b387152f9f64b362",
        "IPY_MODEL_76668d246ea44a5fad1e2e4778816486",
        "IPY_MODEL_fcb2dc9e914a4f759f527d2a6b861de3"
       ],
       "layout": "IPY_MODEL_5f4e4875bbbd4cde94539cf41e0fc1ea"
      }
     },
     "5f4e4875bbbd4cde94539cf41e0fc1ea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6b0923ceed5746769dec2eb94a365b46": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6da410d2b38d4117be37d3e4f45eb502": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "76668d246ea44a5fad1e2e4778816486": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8d2c8198be54485a99d9ce5b20df8686",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a55ba0ab1eda4265980bc3a38e171430",
       "value": 0.0
      }
     },
     "80238127218e402da699101d5ff92806": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "829b1c554687409e8a38caac6b2680d4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8ee3ae325b8a4aad87caab1bdeb87edc",
       "max": 606664.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_20399f12c86f4bc2b90507750334dbf9",
       "value": 606664.0
      }
     },
     "84990e26f3ed4eea8227cec2aa7afaa6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8d2c8198be54485a99d9ce5b20df8686": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "8ee3ae325b8a4aad87caab1bdeb87edc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "96fd5aec8afa46b6838f146252aa969e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6da410d2b38d4117be37d3e4f45eb502",
       "placeholder": "​",
       "style": "IPY_MODEL_f7d63c5921da4aa0ac16f0a943d75816",
       "value": "100%"
      }
     },
     "a18bb87be68f489ea8382075e555131b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a55ba0ab1eda4265980bc3a38e171430": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a81a23932e8b48bc96645fa0d87f60d5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6b0923ceed5746769dec2eb94a365b46",
       "placeholder": "​",
       "style": "IPY_MODEL_84990e26f3ed4eea8227cec2aa7afaa6",
       "value": " 606664/606664 [00:05&lt;00:00, 126759.36it/s]"
      }
     },
     "a97dac9a54524a74b288af347e4be224": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_96fd5aec8afa46b6838f146252aa969e",
        "IPY_MODEL_829b1c554687409e8a38caac6b2680d4",
        "IPY_MODEL_a81a23932e8b48bc96645fa0d87f60d5"
       ],
       "layout": "IPY_MODEL_80238127218e402da699101d5ff92806"
      }
     },
     "af12d8844e20424b9271fe1634dece87": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "dec236423fac461d8c3ee541033a2d52": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ec265fce31414295b387152f9f64b362": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_dec236423fac461d8c3ee541033a2d52",
       "placeholder": "​",
       "style": "IPY_MODEL_a18bb87be68f489ea8382075e555131b",
       "value": ""
      }
     },
     "f7d63c5921da4aa0ac16f0a943d75816": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "fcb2dc9e914a4f759f527d2a6b861de3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0e5fbc3a2a4842c8bcf846822a21ccfa",
       "placeholder": "​",
       "style": "IPY_MODEL_af12d8844e20424b9271fe1634dece87",
       "value": " 0/? [00:00&lt;?, ?it/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
